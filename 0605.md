from pyspark import SparkConf, SparkContext

conf = SparkConf().setMaster("local").setAppName("partitions")
sc = SparkContext(conf=conf)

pairs = sc.parallelize([
    1,2,3,4,2,4,1
]).map(lambda x :(x,x))
pairs.collect()

pairs.partitionBy(2).glom().collect()

#개발자가 직접 파티션 배치 함수 만들 수 있다
pairs.partitionBy(3,lambda x :x%3).glom().collect()

파티션을 만든후에 `persist()`를 하지 않으면 연산에 불릴 때 마다 파티션을 만드는 코드가 계속반복됨
- 셔플링이 반복적으로 일어난다

pairs.partitionBy(2, lambda x :x%2).persist().glom().collect()

sc.stop
